{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#Data preparation\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from keras.preprocessing import sequence\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#define constant variables\n",
    "vocabulary_size = 8000\n",
    "#unknown_token = \"UNKNOWN_TOKEN\" //defined in reduce_words_num_in_tokenized_abstracts function\n",
    "file_name='first1k.txt'\n",
    "\n",
    "df = pd.read_json(path_or_buf=file_name, typ='frame', lines=True)\n",
    "df_reduct = df[[\"abstract\",\"keywords\"]]\n",
    "\n",
    "#delete studies without abstract or keywords\n",
    "df_reduct=df_reduct[df_reduct.abstract.notna() & df_reduct.keywords.notna()] #inkáb loc-ot kellene használni majd\n",
    "\n",
    "df_reduct.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#lowercase, remove sepcial characters and numbers (numbers might be needed), (optional: stemming, stopword removal)\n",
    "df_reduct.abstract=prepare_abstracts_for_tokenization(df_reduct.abstract)\n",
    "\n",
    "#Tokenize the abstracts\n",
    "df_reduct['tokenized_abstracts'] = [nltk.word_tokenize(sent) for sent in df_reduct.abstract]\n",
    "\n",
    "#Get the most frequent words and replace others with unknown_token\n",
    "df_reduct['tokenized_abstracts']=reduce_words_num_in_tokenized_abstracts(df_reduct.tokenized_abstracts, vocabulary_size)\n",
    "\n",
    "#DEPRECATED!!!\n",
    "##split rekords by keywords: deprecated, one should handle more keywords as more label\n",
    "df_splitted=split_by_keywords(df_reduct,\"keywords\")\n",
    "\n",
    "word_set=set().union(*df_reduct.tokenized_abstracts)\n",
    "words_with_index = dict([(w,i) for i,w in enumerate(word_set)])\n",
    "\n",
    "#Replace words with numbers\n",
    "X_train = np.asarray([[words_with_index[w] for w in sent] for sent in df_splitted.tokenized_abstracts])\n",
    "\n",
    "#max_num of words in abstract\n",
    "max_num = len(max(df_splitted.tokenized_abstracts, key=len))\n",
    "\n",
    "#pad input sequences\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_num)\n",
    "\n",
    "############\n",
    "#Label preparation\n",
    "\n",
    "#Get word set\n",
    "y_vocab = df_splitted.keywords.drop_duplicates()\n",
    "\n",
    "#Assign index to word\n",
    "y_index_to_word = [x for x in y_vocab]\n",
    "#y_index_to_word.append(unknown_token)\n",
    "\n",
    "y_word_to_index = dict([(w,i) for i,w in enumerate(y_index_to_word)])\n",
    "\n",
    "y_train = np.asarray([y_word_to_index[str(w)] for w in df_splitted.keywords] )\n",
    "\n",
    "\n",
    "############\n",
    "#Split data set to train and test data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#Create modell\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_vecor_length, input_length=max_num))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "##Functions\n",
    "\n",
    "def prepare_abstracts_for_tokenization(abstracts):\n",
    "    #lower case, remove special characters\n",
    "    abstracts=list(map(lambda x:x.lower(),abstracts))\n",
    "    abstracts=list(map(lambda x:re.sub('[^a-zA-Z ]', ' ', x),abstracts)) #itt kellhetnek a számok is esetleg bizonos spec karakterek is? \n",
    "    abstracts=list(map(lambda x:re.sub('  ', ' ', x),abstracts))\n",
    "\n",
    "    #stemming?\n",
    "\n",
    "    #remove stopwords?\n",
    "    \n",
    "    return abstracts\n",
    "\n",
    "def reduce_words_num_in_tokenized_abstracts(tokenized_abstracts,vocabulary_size):\n",
    "    ##select top n words and replace unknown words with unknown token\n",
    "    #Count word frequency\n",
    "    \n",
    "    unknown_token='UNKNOWN_TOKEN'\n",
    "    \n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_abstracts))\n",
    "\n",
    "    #Get the 8000 most common words\n",
    "    vocab = word_freq.most_common(vocabulary_size-1)\n",
    "    word_set = [x[0] for x in vocab]\n",
    "    word_set.append(unknown_token)\n",
    "\n",
    "    #Replace words missing from word_to_index with unknown token\n",
    "    for i, sent in enumerate(tokenized_abstracts):\n",
    "        tokenized_abstracts[i] = [w if w in word_set else unknown_token for w in sent] #unknown_token to parameter?\n",
    "    \n",
    "    return tokenized_abstracts\n",
    "\n",
    "#Deprecated\n",
    "def split_by_keywords(data_frame,columnname):\n",
    "    df_splitted = pd.DataFrame(columns=data_frame.columns)\n",
    "    for i, element in data_frame.iterrows():\n",
    "        for key_word in element.keywords:\n",
    "            df_splitted=df_splitted.append(element, ignore_index=True)\n",
    "            df_splitted.iloc[-1, df_splitted.columns.get_loc(columnname)] = key_word\n",
    "    return df_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11415748  0.15742187  0.16901349  0.14381915  0.3512613   0.17911656\n",
      "  0.06045374 -0.15670416  0.31205043  0.16865592]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('region', 0.9974913001060486),\n",
       " ('day', 0.9972256422042847),\n",
       " ('attitudes', 0.9969133734703064),\n",
       " ('examination', 0.9969038963317871),\n",
       " ('showing', 0.9968945384025574),\n",
       " ('right', 0.9965606927871704),\n",
       " ('terms', 0.9964362978935242),\n",
       " ('lnm', 0.9964209198951721),\n",
       " ('regions', 0.9963256120681763),\n",
       " ('frequency', 0.9961439371109009)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2vector example\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "file_name='first1k.txt'\n",
    "\n",
    "#read file\n",
    "df = pd.read_json(path_or_buf=file_name, typ='frame', lines=True)\n",
    "df = df[[\"abstract\",\"title\",\"lang\"]]\n",
    "\n",
    "#keep only english studies\n",
    "df=df[df.lang==\"en\"]\n",
    "\n",
    "#remove studies without abstract or title\n",
    "df=df.loc[df.abstract.notna() & df.title.notna()] #inkáb loc-ot kellene használni majd\n",
    "\n",
    "#reset indexes\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#lowercase, remove special characters\n",
    "df.abstract=df.loc[:,'abstract'].str.lower().str.replace('[^a-zA-Z ]', ' ').str.replace('  ', ' ')\n",
    "\n",
    "#tokenize\n",
    "df.abstract=[nltk.word_tokenize(sent) for sent in df.abstract]\n",
    "\n",
    "#apply vectors\n",
    "model = gensim.models.Word2Vec(df.abstract, size=10, window=5)\n",
    "\n",
    "print(model.wv['text'])\n",
    "model.wv.most_similar(\"combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 4283 6361 2521]\n",
      " [   0    0    0 ... 6148 5863 2503]\n",
      " [   0    0    0 ... 6011 4379 1499]\n",
      " ...\n",
      " [   0    0    0 ... 2517 7177 4591]\n",
      " [   0    0    0 ... 5490 2200 7790]\n",
      " [   0    0    0 ... 3122 4060 5291]]\n"
     ]
    }
   ],
   "source": [
    "#word embedding, one integer per word plus padding example\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "file_name='first1k.txt'\n",
    "\n",
    "#read file\n",
    "df = pd.read_json(path_or_buf=file_name, typ='frame', lines=True)\n",
    "df = df[[\"abstract\",\"title\",\"lang\"]]\n",
    "\n",
    "#keep only english studies\n",
    "df=df[df.lang==\"en\"]\n",
    "\n",
    "#remove studies without abstract or title\n",
    "df=df[df.abstract.notna() & df.title.notna()] #inkáb loc-ot kellene használni majd\n",
    "\n",
    "#reset indexes\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#vocab size: one should check vocab sizes of real data and change this value based on that\n",
    "vocab_size = 8000\n",
    "\n",
    "#assign numbers to words, in case of smaller vocab_size than the actual number of words, some words will have the\n",
    "#same number\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in df.abstract]\n",
    "\n",
    "#length of the longest abstract\n",
    "max_length = len(max(encoded_docs, key=len))\n",
    "\n",
    "#fill shorter sentences with 0-s (RNNs need that, other models might not need it)\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\n",
    "\n",
    "print(padded_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
