{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Data preparation\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from keras.preprocessing import sequence\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#define constant variables\n",
    "vocabulary_size = 8000\n",
    "#unknown_token = \"UNKNOWN_TOKEN\" //defined in reduce_words_num_in_tokenized_abstracts function\n",
    "file_name='first1k.txt'\n",
    "\n",
    "df = pd.read_json(path_or_buf=file_name, typ='frame', lines=True)\n",
    "df_reduct = df[[\"abstract\",\"keywords\"]]\n",
    "\n",
    "df_reduct=df_reduct[df_reduct.abstract.notna() & df_reduct.keywords.notna()] #inkáb loc-ot kellene használni majd\n",
    "\n",
    "df_reduct.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#lowercase, remove sepcial characters and numbers (numbers might be needed), (optional: stemming, stopword removal)\n",
    "df_reduct.abstract=prepare_abstracts_for_tokenization(df_reduct.abstract)\n",
    "\n",
    "#Tokenize the abstracts\n",
    "df_reduct['tokenized_abstracts'] = [nltk.word_tokenize(sent) for sent in df_reduct.abstract]\n",
    "\n",
    "#Get the most frequent words and replace others with unknown_token\n",
    "df_reduct['tokenized_abstracts']=reduce_words_num_in_tokenized_abstracts(df_reduct.tokenized_abstracts, vocabulary_size)\n",
    "\n",
    "#DEPRECATED!!!\n",
    "##split rekords by keywords: deprecated, one should handle more keywords as more label\n",
    "df_splitted=split_by_keywords(df_reduct,\"keywords\")\n",
    "\n",
    "word_set=set().union(*df_reduct.tokenized_abstracts)\n",
    "words_with_index = dict([(w,i) for i,w in enumerate(word_set)])\n",
    "\n",
    "#Replace words with numbers\n",
    "X_train = np.asarray([[words_with_index[w] for w in sent] for sent in df_splitted.tokenized_abstracts])\n",
    "\n",
    "#max_num of words in abstract\n",
    "max_num = len(max(df_splitted.tokenized_abstracts, key=len))\n",
    "\n",
    "#pad input sequences\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_num)\n",
    "\n",
    "############\n",
    "#Label preparation\n",
    "\n",
    "#Get word set\n",
    "y_vocab = df_splitted.keywords.drop_duplicates()\n",
    "\n",
    "#Assign index to word\n",
    "y_index_to_word = [x for x in y_vocab]\n",
    "#y_index_to_word.append(unknown_token)\n",
    "\n",
    "y_word_to_index = dict([(w,i) for i,w in enumerate(y_index_to_word)])\n",
    "\n",
    "y_train = np.asarray([y_word_to_index[str(w)] for w in df_splitted.keywords] )\n",
    "\n",
    "\n",
    "############\n",
    "#Split data set to train and test data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Create modell\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_vecor_length, input_length=max_num))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "##Functions\n",
    "\n",
    "def prepare_abstracts_for_tokenization(abstracts):\n",
    "    #lower case, remove special characters\n",
    "    abstracts=list(map(lambda x:x.lower(),abstracts))\n",
    "    abstracts=list(map(lambda x:re.sub('[^a-zA-Z ]', ' ', x),abstracts)) #itt kellhetnek a számok is esetleg bizonos spec karakterek is? \n",
    "    abstracts=list(map(lambda x:re.sub('  ', ' ', x),abstracts))\n",
    "\n",
    "    #stemming?\n",
    "\n",
    "    #remove stopwords?\n",
    "    \n",
    "    return abstracts\n",
    "\n",
    "def reduce_words_num_in_tokenized_abstracts(tokenized_abstracts,vocabulary_size):\n",
    "    ##select top n words and replace unknown words with unknown token\n",
    "    #Count word frequency\n",
    "    \n",
    "    unknown_token='UNKNOWN_TOKEN'\n",
    "    \n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_abstracts))\n",
    "\n",
    "    #Get the 8000 most common words\n",
    "    vocab = word_freq.most_common(vocabulary_size-1)\n",
    "    word_set = [x[0] for x in vocab]\n",
    "    word_set.append(unknown_token)\n",
    "\n",
    "    #Replace words missing from word_to_index with unknown token\n",
    "    for i, sent in enumerate(tokenized_abstracts):\n",
    "        tokenized_abstracts[i] = [w if w in word_set else unknown_token for w in sent] #unknown_token to parameter?\n",
    "    \n",
    "    return tokenized_abstracts\n",
    "\n",
    "#Deprecated\n",
    "def split_by_keywords(data_frame,columnname):\n",
    "    df_splitted = pd.DataFrame(columns=data_frame.columns)\n",
    "    for i, element in data_frame.iterrows():\n",
    "        for key_word in element.keywords:\n",
    "            df_splitted=df_splitted.append(element, ignore_index=True)\n",
    "            df_splitted.iloc[-1, df_splitted.columns.get_loc(columnname)] = key_word\n",
    "    return df_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
